{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\personnel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\personnel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\personnel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelques fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_terms = [\n",
    "    'douleur abdominale', 'douleur thoracique', 'douleur lombaire', 'conjonctive coloree',\n",
    "    'tension arterielle', 'frequence cardiaque', 'frequence respiratoire'\n",
    "]\n",
    "stop_words = set(stopwords.words('french'))\n",
    "def preprocess_text(text, specific_terms=specific_terms):\n",
    "    # Supprimer les caractères spéciaux et la ponctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer les accents\n",
    "    text = unidecode.unidecode(text)\n",
    "    # Supprimer les mots vides (stop words)\n",
    "    mots_a_exclure = ['a', 'bilan', 'evoluant', 'debut', 'remonterait', 'marque', 'mois', 'linstallation', 'dune', 'patient', 'gauche', 'droit', 'droite', 'semaine', 'semaines', 'jour', 'jours', 'jrs', 'contexte', 'ras', 'consulte', 'consultation', 'jours', 'jrs', 'avoir','deux', 'selon', 'contre','vendredi','plus','depuis','ans','entre','ont', 'vos', 'aurions', 'ne', 'es', 'fussent', 'auriez', 'les', 'au', 'aurait', 'aux', 'seraient', 'samedi','récit', 'plusieurs', 'avons', 'aura', 'ayons', 'avez', 'as', 'ayants', 'mais', 'n', 'le', 'eut', 'aies', 'ses', 'aie', 'avions', 'serez', 'du', 'je', 'auront', 'eussiez', 'ta', 'étés', 'étaient', 'serai', 'avaient', 'serait', 'étiez', 'étais', 'fussiez', 'lui', 'elle', 'eûmes', 'ayant', 'seras', 'était', 'aurais', 'aurez', 'fusse', 'fussions', 'on', 'leur', 'sont', 'pas', 'ayantes', 'eue', 'fût', 'vous', 'sera', 'même', 'y', 'fûtes', 'de', 'eurent', 'eu', 'ou', 'des', 'soit', 'qu', 'il', 'moi', 'et', 'ai', 'soyons', 'ils', 'ma', 'que', 'nos', 'la', 'tes', 'ces', 'eusse', 'ait', 'se', 'ton', 'aviez', 'eussions', 'mes', 'eussent', 'pour', 'fut', 'ce', 'eûtes', 'nous', 'l', 'sommes', 'c', 't', 'un', 'ayez', 'eues', 'm', 'serions', 'seriez', 'sa', 'j', 'furent', 'avait', 's', 'en', 'd', 'serais', 'étée', 'à', 'qui', 'votre', 'aurai', 'par', 'êtes', 'eux', 'étantes', 'tu', 'sois', 'fus', 'avec', 'sur', 'eusses', 'une', 'eût', 'est', 'toi', 'soyez', 'aurons', 'me', 'étées', 'seront', 'avais', 'son', 'mon', 'étant', 'auraient', 'aient', 'été', 'étants', 'étante', 'dans', 'suis', 'notre', 'te', 'eus', 'étions', 'fûmes', 'auras', 'soient', 'fusses', 'ayante', 'serons']  # Remplacez par votre liste de mots à exclure\n",
    "    \n",
    "    # Remplacer les expressions spécifiques par un token unique\n",
    "    for term in specific_terms:\n",
    "        text = re.sub(r'\\b' + term + r'\\b', term.replace(' ', '_'), text)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    filtered_text = [mot for mot in words if mot not in stop_words and mot not in mots_a_exclure]\n",
    "    \n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    \n",
    "    return ' '.join(lemmatized_text)\n",
    "    #return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequence des mots\n",
    "def frequence_mot(texte, nb=None):   \n",
    "    # tokenisation du texte\n",
    "    mots_tokenise = word_tokenize(' '.join(texte))\n",
    "    print(mots_tokenise)\n",
    "        \n",
    "    # Compter les mots fréquents dans les titres\n",
    "    mots_freq= FreqDist(mots_tokenise)\n",
    "    # Obtenir les mots fréquents avec leurs occurrences\n",
    "    mots_occurrences = mots_freq.most_common(nb)\n",
    "    # Créer un DataFrame à partir des mots fréquents avec leurs occurrences\n",
    "    df = pd.DataFrame(list(mots_occurrences), columns=['Mot', 'Occurrences'])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangling(df):\n",
    "    # Suppression des colonnes vides\n",
    "    vides = ['Fréquence cardiaque (FC)', 'Fréquence respiratoire (FR)', 'Conscience (SCG)', 'Glycemie', 'allergies_imm',\n",
    "            'autres_gyn', 'autres_imm', 'chirurgie']\n",
    "    df.drop(vides, axis=1, inplace=True)\n",
    "\n",
    "    selected_data = ['clinique', 'motif_consultation', 'enquete_systeme', 'exam_physique', 'histoire_maladie',\n",
    "                     'IMC', 'IT', 'Poids', 'Saturation en O2', 'Taille', 'Température',\n",
    "                    'Tension Arterielle (BD)', 'Tension Arterielle (BG)', 'age', 'sexe'\n",
    "                    ]\n",
    "    df = df[selected_data]\n",
    "    # Selection des colonnes categorielles\n",
    "    data_category = df[df.select_dtypes(include=['object']).columns].fillna('RAS')\n",
    "    \n",
    "    # Reformattage des données mal renseignées\n",
    "    for i, row in df.iterrows():\n",
    "        if row['age'] > 999:\n",
    "            df['age'][i] = 2024 - row['age']\n",
    "            \n",
    "        if row['age'] > 70 and row['Taille'] < 75:\n",
    "            df['Taille'][i] = row['Taille'] + 100\n",
    "\n",
    "        if row['Saturation en O2'] > 100 :\n",
    "            df['Saturation en O2'][i] = 99\n",
    "        \n",
    "    # Remplissage des valeurs manquantes\n",
    "    df['Température'].fillna(37, inplace=True)\n",
    "    #df['alcool_imm'].fillna(0, inplace=True)\n",
    "    df['Saturation en O2'].fillna(99, inplace=True)\n",
    "\n",
    "    # Remplissage des valeurs numeriques vides par leurs moyens \n",
    "    #for col in df.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns:\n",
    "    #    df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "    # Suppressions des valeurs considerées inutiles \n",
    "    for i, row in data_category.iterrows():\n",
    "        str(row[df.select_dtypes(include=['object']).columns]).replace('+','')\n",
    "        str(row[df.select_dtypes(include=['object']).columns]).replace('/',' ')\n",
    "        str(row[df.select_dtypes(include=['object']).columns]).replace(':',' ')\n",
    "        str(row[df.select_dtypes(include=['object']).columns]).replace('bilan',' ')\n",
    "        str(row[df.select_dtypes(include=['object']).columns]).replace('Bilan',' ')\n",
    "\n",
    "    # Appliquer le prétraitement aux colonnes textuelles\n",
    "    #data_category = data_category.astype(str).apply(preprocess_text)\n",
    "    data_category['clinique'] = data_category['clinique'].astype(str).apply(preprocess_text)\n",
    "    data_category['motif_consultation'] = data_category['motif_consultation'].astype(str).apply(preprocess_text)\n",
    "    data_category['histoire_maladie'] = data_category['histoire_maladie'].astype(str).apply(preprocess_text)\n",
    "    data_category['enquete_systeme'] = data_category['enquete_systeme'].astype(str).apply(preprocess_text)\n",
    "    data_category['exam_physique'] = data_category['exam_physique'].astype(str).apply(preprocess_text)\n",
    "    #data_category['conduite_a_tenir'] = data_category['conduite_a_tenir'].astype(str).apply(preprocess_text)\n",
    "    #data_category['ethnie'] = data_category['ethnie'].astype(str).apply(preprocess_text)\n",
    "    data_category['sexe'] = data_category['sexe'].astype(str).apply(preprocess_text)\n",
    "    \n",
    "    # Regroupement des colonnes textuelles en une colonne unique\n",
    "    text_data = data_category[['clinique', 'motif_consultation', 'enquete_systeme', 'exam_physique', 'histoire_maladie']]\n",
    "    text_data['symptomes'] = ' '\n",
    "    for i, row in text_data.iterrows():\n",
    "        mots = []\n",
    "        mots_vide = ['ras', 'ra', 'RAS']\n",
    "        mots += [mot for mot in row['enquete_systeme'].split(',') if mot not in mots and  mot not in mots_vide]\n",
    "        mots += [mot for mot in row['exam_physique'].split(',') if mot not in mots and mot not in mots_vide]\n",
    "        mots += [mot for mot in row['histoire_maladie'].split(',') if mot not in mots and mot not in mots_vide]\n",
    "        mots += [mot for mot in row['motif_consultation'].split(',') if mot not in mots and mot not in mots_vide]\n",
    "        mots += [mot for mot in row['clinique'].split(',') if mot not in mots and mot not in mots_vide]\n",
    "\n",
    "        # mots.append(str(row['target']))\n",
    "\n",
    "        for mot in mots:\n",
    "            text_data['symptomes'][i] += ' ' + mot\n",
    "\n",
    "    # Remplacement des termes specifiques\n",
    "    def replace_specific_terms(text, replacements):\n",
    "        for old_term, new_term in replacements.items():\n",
    "            text = re.sub(r'\\b' + re.escape(old_term) + r'\\b', new_term, text)\n",
    "        return text\n",
    "    \n",
    "    # Dictionnaire des termes spécifiques à remplacer\n",
    "    replacements = {\n",
    "        'doulur': 'douleur',\n",
    "        'abdominal': 'abdominale',\n",
    "        'cephales': 'cephale',\n",
    "        'douleurs': 'douleur',\n",
    "        'vertiges': 'vertige',\n",
    "        'conjonctives': 'conjonctive',\n",
    "        'conjonction': 'conjonctive',\n",
    "        'scleres': 'sclere',\n",
    "    }\n",
    "\n",
    "    # Remplacer les termes spécifiques dans la colonne symptomes\n",
    "    text_data['symptomes'] = text_data['symptomes'].apply(lambda x: replace_specific_terms(x, replacements))\n",
    "\n",
    "    cleaned_data = df[['IMC', 'IT', 'Poids', 'Saturation en O2', 'Taille', 'Température',\n",
    "                    'Tension Arterielle (BD)', 'Tension Arterielle (BG)', 'age']\n",
    "                    ]\n",
    "    cleaned_data['sexe'] = data_category['sexe']\n",
    "    cleaned_data['symptomes'] = text_data['symptomes']\n",
    "\n",
    "    # Pretraitement de la colonne symptome\n",
    "    my_stemmer = LancasterStemmer()\n",
    "    stemmed = [[my_stemmer.stem(word) for word in review.split()] for review in text_data['symptomes'] ]\n",
    "    stemmed_concat = [' '.join(review) for review in stemmed]\n",
    "\n",
    "    \"\"\" # Suppression des colonnes pas très importantes\n",
    "    df.drop(['id', 'motif_consultation', 'histoire_maladie', 'clinique', 'enquete_systeme', 'exam_physique', 'conduite_a_tenir',\n",
    "                'ethnie', 'profession', 'quartier', 'religion', 'statut_matrimonial', 'medicaux', 'alcool_imm', 'target', 'symptomes', \n",
    "                'diagnostique', ], axis=1, inplace=True)  # A supprimer\n",
    "    \n",
    "    \n",
    "    # Encodage des données\n",
    "    le = LabelEncoder()\n",
    "    df['sexe'] = le.fit_transform(df['sexe'])\n",
    "\n",
    "    # Normaliser les données numériques\n",
    "    scaler = StandardScaler()\n",
    "    data_num_scaled = scaler.fit_transform(df) \n",
    "\n",
    "    # Séparer les textes et les labels\n",
    "    texts = text_data['symptomes']\n",
    "\n",
    "    ### Vectoriser les textes \n",
    "    # Vectorisation TF-IDF\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    text_tranform = tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Combiner les caractéristiques textuelles et numériques pour les ensembles d'entraînement et de test\n",
    "    X_combined = sp.hstack((text_tranform, data_num_scaled))  \"\"\"\n",
    "\n",
    "    return cleaned_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour serialiser et sauvegarder la fonction preprocess\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def preprocess_num_data():\n",
    "    # categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    # Créer une pipeline pour les transformations\n",
    "    pipeline = Pipeline([\n",
    "        # Encodage des variables catégorielles\n",
    "        ('encoder', OrdinalEncoder()),\n",
    "        # Imputation des valeurs manquantes\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        # Mise à l'échelle des caractéristiques numériques\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    # Sauvegarde de la fonction sous pkl\n",
    "    pk.dump(pipeline, open(f'../models/pipeline_num_preprocess.pkl', 'wb'))\n",
    "    return pipeline\n",
    "\n",
    "def preprocess_text_data():\n",
    "    pipeline = Pipeline([\n",
    "         # Pretraitement des texts avec tfid\n",
    "        ('tfid', TfidfVectorizer()),\n",
    "    ])\n",
    "    pk.dump(pipeline, open(f'../models/pipeline_text_preprocess.pkl', 'wb'))\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour charger la fonction sérialisée\n",
    "def load_preprocess_data(filepath):\n",
    "    preprocess_function = pk.load(open(filepath, 'rb'))\n",
    "    return preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\personnel\\AppData\\Local\\Temp\\ipykernel_828\\918510646.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(vides, axis=1, inplace=True)\n",
      "C:\\Users\\personnel\\AppData\\Local\\Temp\\ipykernel_828\\918510646.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data['sexe'] = data_category['sexe']\n",
      "C:\\Users\\personnel\\AppData\\Local\\Temp\\ipykernel_828\\918510646.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data['symptomes'] = text_data['symptomes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14      courbature polyarthralgie cephale toux beg c...\n",
       "Name: symptomes, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donnees = pd.read_csv('../data/Patients_data.csv')\n",
    "clean_data = wrangling(donnees[14:15])\n",
    "clean_data['symptomes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num_preprocessed = preprocess_num_data()\n",
    "data_num = data_num_preprocessed.fit_transform(clean_data.drop(['symptomes'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_text_preprocessed = preprocess_text_data()\n",
    "#data_text = data_text_preprocessed.fit_transform(clean_data['symptomes'])\n",
    "data_text_p = load_preprocess_data('../models/pipeline_text_preprocess1.pkl')\n",
    "data_text = data_text_p.transform(clean_data['symptomes'])\n",
    "\n",
    "data_num_p = load_preprocess_data('../models/pipeline_num_preprocess1.pkl')\n",
    "data_num = data_num_p.fit_transform(clean_data.drop(['symptomes'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 677)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_num.shape\n",
    "data_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x687 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined = sp.hstack((data_text, data_num))\n",
    "X_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 677)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importer modele de prediction\n",
    "model = load_preprocess_data('../models/SVM.pkl')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;encoder&#x27;, OrdinalEncoder()),\n",
       "                (&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                (&#x27;scaler&#x27;, StandardScaler()), (&#x27;tfid&#x27;, TfidfVectorizer())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;encoder&#x27;, OrdinalEncoder()),\n",
       "                (&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                (&#x27;scaler&#x27;, StandardScaler()), (&#x27;tfid&#x27;, TfidfVectorizer())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OrdinalEncoder</label><div class=\"sk-toggleable__content\"><pre>OrdinalEncoder()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('encoder', OrdinalEncoder()),\n",
       "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
       "                ('scaler', StandardScaler()), ('tfid', TfidfVectorizer())])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = load_preprocess_data('../models/pipeline_preprocess.pkl')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer modele de prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
